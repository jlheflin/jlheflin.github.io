{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Jacob Heflin's GitHub Site","text":"<p>Please see the left bar for options!</p>"},{"location":"about/","title":"Neve tum aratri facere iterum","text":""},{"location":"about/#syenites-vaccam-remansit-gesserit-taedia-armeniae-inque","title":"Syenites vaccam remansit gesserit taedia Armeniae inque","text":"<p>Lorem markdownum gratia; aufer, ait infelix dum corpus nec erat ille sustulit cruribus cum, venenis e. Nec atque et omnem servantis taurum adolentur refert; orbum dolore, Iasone auro. Iuventus solo verum ait quodam molles orbe aufert quae.</p> <ul> <li>Sine usque in</li> <li>Candida movere</li> <li>Deus coryli coissent medius</li> <li>Ab saevior Pelasgos per et Venus est</li> </ul> <p>Conscia pristina nigra. Limite cur, Cancrum ingenti movere iunxit; ipsam tibi mox at unguibus olim infelix Tartara.</p>"},{"location":"about/#fraga-dabitur-mens-terris","title":"Fraga dabitur mens terris","text":"<p>Mihi est nubila usus procul nymphae tubicen album, intendit a placere prehendit. Oraque substitit flere mihi gerat posset non, enim ante, ire. Rictuque famuli. Apta sed Cecropidis fretum causam generis rerum oraque resonantia gloria percepit nivibus coeunt: fallunt quid amicas, hoc!</p> <p>Est pia: coniugis: vix pavet, Nini et dei quaeque neque, est super quibus. In in et deus intellecta aquis, qua Asida, studiosior Tyrioque utque, annos utinam digna. Deus repleri Menoetae, fuge illo aequum; locorum quam modo oscula hunc! Has manus ad milite mutatur habuit crederet ne petiere, dis.</p> <p>Moles sublimis! Spectare in equos ferunt quae, alta iam inquit sibi caelo, nacta latus. Malorum ducebas quod patior vanis crescitque quarum. Sumptas suarum dimittere quoque e viderat, dum petens veniebat, contigerant carinas Letoia exit fallere terras denique fertur: pias.</p> <p>Stipite mea, in, quicquid inductus laeva; aere donec. Laborem sororum, prisci cursus, quaque suas, cornibus fortibus; et. Ero vulnere aperire et rabies cacumine volucris contendere nitor, ut iter, temperat domusque. Ingreditur dedit mundus unde aper veli qui novitate, loco.</p>"},{"location":"nwchemex_install_instructions/","title":"NWChemEx Install Instructions","text":""},{"location":"nwchemex_install_instructions/#setup","title":"Setup","text":"<p>Typically, setup involves having to obtain prerequisite software to be able to compile the NWChemEx stack. This includes the libraries Blas/OpenBlas and Libint2. For Blas/OpenBlas, the software can be downloaded via a package manager that has a package available that provides the library. Usually when a package is installed via the main package manager of an operating system, those libraries end up in the <code>/usr/lib</code> directory. This folder is usually checked by default by our current build system, CMake, to find the prerequisite software.</p> <p>Personally, I prefer putting libraries into <code>/home/jacob/Libraries</code> on my own system:</p> <pre><code>cd /home/jacob/Libraries\ncurl -L -O https://github.com/evaleev/libint/releases/download/v2.6.0/libint-2.6.0.tgz\ntar xvf libint-2.6.0.tgz\ncd libint-2.6.0\n</code></pre> <p>Knowing where the library is installed is important if you plan to download and compile the libraries from source. </p>"},{"location":"nwchemex_install_instructions/#libint2-build","title":"Libint2 Build","text":"<p>This tells <code>cmake</code> that the current source directory is the current file (-S), that we want the configuration and build to happen in the \"build\" directory (<code>cmake</code> will create this folder if it doesn't exist), and that we would like to set the <code>CMAKE_INSTALL_PREFIX</code> to <code>`pwd`/install,</code> which will resolve to <code>`/home/jacob/Libraries/libint-2.6.0/install</code>, meaning that the result of the build will be installed to that path.</p> <pre><code>cmake -S . -B build -DCMAKE_INSTALL_PREFIX=`pwd`/install\n</code></pre> <p>If needed, you can confirm that the <code>CMAKE_INSTALL_PREFIX</code> variable was set via <code>ccmake</code>, but using <code>ccmake</code> to configure and generate the build usually results in undefined behavior for the build, so I would just use <code>ccmake</code> to confirm that variables have been set correctly.</p> <pre><code># OPTIONAL\nccmake build # Use 'q' to quit\n</code></pre> <p>Then the build begins! Specify that the build directory is build, the target is to install the library, and that we would like to use 4 cores to compile the code.</p> <pre><code>cmake --build build --target install --parallel 4\n</code></pre> <p>At the end of compilation, you should have a <code>install/</code> folder in the libint-2.6.0 folder, which will have <code>lib/</code>, <code>include/</code>, and <code>share/</code>. This is important to know for our <code>CMAKE_PREFIX_PATH</code> variable later for building NWChemEx.</p>"},{"location":"nwchemex_install_instructions/#blasopenblas","title":"Blas/OpenBlas","text":"<p>This library is typically the easiest to install, I usually go through the main package manager of the operating system. This would be the following</p>"},{"location":"nwchemex_install_instructions/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code>sudo apt isntall libopenblas-dev\n</code></pre> <p>As long as you are able to point to <code>libopenblas.so</code> via <code>CMAKE_PREFIX_PATH</code>, this should work fine.</p>"},{"location":"nwchemex_install_instructions/#obtaining-nwchem","title":"Obtaining NWChem","text":"<p>NWChemEx depends on a few executable programs, one of which is NWChem. You can obtain NWChem a multitude of ways.</p>"},{"location":"nwchemex_install_instructions/#ubuntudebian_1","title":"Ubuntu/Debian","text":"<pre><code>sudo apt install nwchem\n</code></pre>"},{"location":"nwchemex_install_instructions/#homebrew","title":"Homebrew","text":"<pre><code>brew install nwchem\n</code></pre>"},{"location":"nwchemex_install_instructions/#nix-package-manager","title":"Nix Package Manager","text":"<pre><code>nix-env -iA nixpkgs.nwchem\n</code></pre> <p>The Homebrew and Nix versions of NWChem are build with OpenMPI, which requires it to be run via <code>mpirun nwchem -np &lt;number-of-cores&gt; nwchem input.nw</code>, but the implementation in the underlying framework QCEngine doesn't handle this well. If you install a version of NWChem that requires <code>mpirun</code>, you should use the following script and have it in your path:</p> <pre><code>#!/usr/bin/env bash\nREAL_NWCHEM=\"&lt;path-to-nwchem&gt;\" # eg /home/jacob/Applications/nwchem/bin/LINUX64/nwchem DO NOT JUST DO nwchem\necho \"Running NWChem located at: $REAL_NWCHEM\"\nif [[ \"$0\" == \"$REAL_NWCHEM\" ]]; then\n    echo \"Error: Wrapper is calling itself!\" &gt;&amp;2\n    exit 1\nfi\nexec mpirun \"$REAL_NWCHEM\" \"$@\"\n</code></pre> <p>This essentially tricks QCEngine into running NWChem with <code>mpirun</code>.</p>"},{"location":"nwchemex_install_instructions/#python-environment","title":"Python Environment","text":"<p>I like to set up my python environments in <code>/home/jacob/Environments/</code>, but the path doesn't necessarily matter so long as your source it BEFORE you configure NWChemEx via <code>cmake</code> (otherwise the interpreter path for NWChemEx will be set to the incorrect path)</p> <pre><code>cd /home/jacob/Environments\npython3 -m venv nwchemex-env\n# I prefer to source from an absolue path /home/jacob/Environments/nwchemex-env/bin/activate, but\n# this way is fine as well\nsource ./nwchemex-env/base/bin/activate\n</code></pre> <p>The necessary python packages that we need are <code>qcelemental</code>, <code>qcengine</code>, <code>networkx</code>, and <code>ase</code>:</p> <pre><code># With the environment activated\npip install qcelemental qcengine networkx ase\n</code></pre>"},{"location":"nwchemex_install_instructions/#nwchemex-build","title":"NWChemEx Build","text":"<p>Now we can go on to building NWChemEx! We typically use a <code>toolchain.cmake</code> file to specify certain variables to pass to cmake at configuration, here is mine:</p> <pre><code># GCC Setup\nset(CMAKE_C_COMPILER   gcc)\nset(CMAKE_CXX_COMPILER g++)\nset(MPI_C_COMPILER     mpicc)\nset(MPI_CXX_COMPILER   mpic++)\n\n# Options\nset(CMAKE_POSITION_INDEPENDENT_CODE TRUE)\nset(BUILD_SHARED_LIBS TRUE)\nset(BUILD_TESTING TRUE)\nset(CMAKE_EXPORT_COMPILE_COMMANDS TRUE) # Useful for LSPs like Clangd\nset(CMAKE_POLICY_VERSION_MINIMUM 3.5) # Only needed if CMake version is too new\nset(ENABLE_SIGMA ON)\n\n# List directories for dependencies you have installed in non-standard\n# locations. For example:\nset(CMAKE_PREFIX_PATH \"/home/jacob/Libraries/libint-2.9.0/install:/usr/lib\") # /usr/lib should already be found\nset(CMAKE_CXX_STANDARD 17)\n\n# BLAS/LAPACK\nset(ENABLE_SCALAPACK ON)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DOMPI_SKIP_MPICXX\")\n</code></pre> <p>Some things to note, if you need to install a version of ChemCache that has more basis sets available, you will most likely need to clone ChemCache and check out the <code>generated_data</code> branch:</p> <pre><code># I like to keep outside projects in my /home/jacob/Projects folder\ncd /home/jacob/Projects\ngit clone https://github.com/NWChemEx/ChemCache.git\ncd chemcache\ngit checkout generated_data\n</code></pre> <p>You can confirm that the generated data is available if you go to the <code>src/chemcache/bases</code> folder and see more than just <code>sto_dash_3g</code> in the folder. Then you will want to add the following line to the <code>toolchain.cmake</code> file:</p> <pre><code>set(FETCHCONTENT_SOURCE_DIR_CHEMCACHE \"/home/jacob/Projects/ChemCache\")\n</code></pre> <p>Now we are ready to configure, confirm, and build NWChemEx. We can run the following command to configure the NWChemEx build (MAKE SURE YOU HAVE SOURCED YOUR PYTHON ENVIRONMENT!):</p> <pre><code>cmake -S . -B build -DCMAKE_INSTALL_PREFIX=`pwd`/install \\\n                    -DNWX_MODULE_DIR=`pwd`/install \\\n                    -DCMAKE_TOOLCHAIN_FILE=toolchain.cmake\n</code></pre> <p>CMake will configure the build and download any sources that it can't find. After a successful configuration, we will want to confirm that our variables were set via <code>ccmake</code>, you will want to confirm <code>CMAKE_INSTALL_PREFIX</code>, <code>CMAKE_PREFIX_PATH</code>, and <code>FETCHCONTENT_SOURCE_DIR_CHEMCACHE</code> if set.</p> <p>Now, it is time to build the NWChemEx stack:</p> <pre><code>cmake --build build --target install --parallel 4\n</code></pre> <p>You can use more than 4 cores if needed, but if you are building the full ChemCache with the <code>generated_data</code> branch, this may cause the memory to be exceeded on your system. 4 cores is pretty safe. If you are building the full ChemCache with generated data, this build process will take a while.</p> <p>Once the build is complete, you should have a folder named <code>install/</code> in the NWChemEx directory. We need to do a bit of clean up since the NWChemEx install process isn't fully working yet.</p> <p>First, let's check out what is in the <code>install/</code> folder:</p> <pre><code>Permissions Size User  Date Modified Name\ndrwxr-xr-x     - jacob 10 Jun 17:59  bin\ndrwxr-xr-x     - jacob 10 Jun 17:59  friendzone\ndrwxr-xr-x     - jacob 10 Jun 17:59  include\ndrwxr-xr-x     - jacob 10 Jun 17:59  lib\ndrwxr-xr-x     - jacob 10 Jun 17:59  nwchemex\ndrwxr-xr-x     - jacob 10 Jun 17:59  share\n.rwxr-xr-x  210k jacob 10 Jun 17:59  chemcache.so\n.rwxr-xr-x  3.0M jacob 10 Jun 17:52  chemist.so\n.rwxr-xr-x  210k jacob 10 Jun 17:52  integrals.so\n.rwxr-xr-x  210k jacob 10 Jun 17:49  nux.so\n.rwxr-xr-x  512k jacob 10 Jun 17:44  parallelzone.so\n.rwxr-xr-x  914k jacob 10 Jun 17:45  pluginplay.so\n.rwxr-xr-x  232k jacob 10 Jun 17:53  scf.so\n.rwxr-xr-x  2.7M jacob 10 Jun 17:48  simde.so\n.rwxr-xr-x  298k jacob 10 Jun 17:47  tensorwrapper.so\n</code></pre> <p>The result of setting <code>-DNWX_MODULE_DIRECTORY=`pwd`/install</code> is that the pybind11 python modules (denoted by <code>.so</code> suffix, referencing <code>parallelzone.so</code>, <code>chemcache.so</code>, etc.) as well as the <code>friendzone</code> and <code>nwchemex</code> python modules are installed here. When you plan to start interacting with the NWChemEx stack via <code>python</code>, this is the path where you will set <code>PYTHONPATH</code> environment variable. This will allow <code>python</code> to find these modules, so they can be imported like usual in a python setting (i.e. <code>import parallelzone</code>).</p> <p>Now, let's go into the <code>lib/</code> folder:</p> <pre><code>Permissions Size User  Date Modified Name\ndrwxr-xr-x     - jacob 10 Jun 17:59  chemcache\ndrwxr-xr-x     - jacob 10 Jun 17:59  chemist\ndrwxr-xr-x     - jacob 10 Jun 17:59  cmake\ndrwxr-xr-x     - jacob 10 Jun 17:59  integrals\ndrwxr-xr-x     - jacob 10 Jun 17:59  parallelzone\ndrwxr-xr-x     - jacob 10 Jun 17:59  pkgconfig\ndrwxr-xr-x     - jacob 10 Jun 17:59  pluginplay\ndrwxr-xr-x     - jacob 10 Jun 17:59  sigma\ndrwxr-xr-x     - jacob 10 Jun 17:59  simde\ndrwxr-xr-x     - jacob 10 Jun 17:59  tensorwrapper\ndrwxr-xr-x     - jacob 10 Jun 17:59  utilities\n.rwxr-xr-x  691k jacob 10 Jun 17:45  libexchcxx.so\nlrwxrwxrwx     - jacob 10 Jun 17:59  libfort.so -&gt; libfort.so.0.4\nlrwxrwxrwx     - jacob 10 Jun 17:59  libfort.so.0.4 -&gt; libfort.so.0.4.2\n.rwxr-xr-x  118k jacob 10 Jun 17:43  libfort.so.0.4.2\n.rw-r--r--  6.0M jacob 10 Jun 17:46  libgauxc.a\nlrwxrwxrwx     - jacob 10 Jun 17:59  libspdlog.so -&gt; libspdlog.so.1.11\nlrwxrwxrwx     - jacob 10 Jun 17:59  libspdlog.so.1.11 -&gt; libspdlog.so.1.11.0\n.rwxr-xr-x  748k jacob 10 Jun 17:44  libspdlog.so.1.11.0\nlrwxrwxrwx     - jacob 10 Jun 17:59  libxc.so -&gt; libxc.so.12\n.rwxr-xr-x   13M jacob 10 Jun 17:45  libxc.so.12\n</code></pre> <p>In order to be able to use the build libraries for use with both the C++ and Python interfaces, we need to make it a bit easier to find the libraries. I do this by symlinking all the libraries with the <code>chemcache</code>, <code>chemist</code>, <code>integrals</code>, <code>parallelzone</code>, <code>pluginplay</code>, <code>tensorwrapper</code>, and <code>utilities</code> folders to the <code>lib/</code> directory. You can do this via the following script from the <code>lib/</code> directory:</p> <pre><code>#!/usr/bin/env bash\nln -s ./chemcache/libchemcache.so.1 .\nln -s ./chemist/libchemist.so.1 .\nln -s ./integrals/libintegrals.so.0 .\nln -s ./parallelzone/libparallelzone.so.0 .\nln -s ./pluginplay/libpluginplay.so.1 .\nln -s ./tensorwrapper/libtensorwrapper.so.0 .\nln -s ./utilities/libutilities.so.0 .\n</code></pre> <p>Running the script should result in the following files in the <code>lib/</code> directory:</p> <pre><code>Permissions Size User  Group Date Modified Git Name\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I chemcache\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I chemist\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I cmake\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I integrals\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I parallelzone\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I pkgconfig\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I pluginplay\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I sigma\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I simde\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I tensorwrapper\ndrwxr-xr-x     - jacob jacob 10 Jun 17:59   -I utilities\nlrwxrwxrwx     - jacob jacob 12 Jun 16:41   -I libchemcache.so.1 -&gt; ./chemcache/libchemcache.so.1\nlrwxrwxrwx     - jacob jacob 12 Jun 16:41   -I libchemist.so.1 -&gt; ./chemist/libchemist.so.1\n.rwxr-xr-x  691k jacob jacob 10 Jun 17:45   -I libexchcxx.so\nlrwxrwxrwx     - jacob jacob 10 Jun 17:59   -I libfort.so -&gt; libfort.so.0.4\nlrwxrwxrwx     - jacob jacob 10 Jun 17:59   -I libfort.so.0.4 -&gt; libfort.so.0.4.2\n.rwxr-xr-x  118k jacob jacob 10 Jun 17:43   -I libfort.so.0.4.2\n.rw-r--r--  6.0M jacob jacob 10 Jun 17:46   -I libgauxc.a\nlrwxrwxrwx     - jacob jacob 12 Jun 16:41   -I libintegrals.so.0 -&gt; ./integrals/libintegrals.so.0\nlrwxrwxrwx     - jacob jacob 12 Jun 16:41   -I libparallelzone.so.0 -&gt; ./parallelzone/libparallelzone.so.0\nlrwxrwxrwx     - jacob jacob 12 Jun 16:41   -I libpluginplay.so.1 -&gt; ./pluginplay/libpluginplay.so.1\nlrwxrwxrwx     - jacob jacob 10 Jun 17:59   -I libspdlog.so -&gt; libspdlog.so.1.11\nlrwxrwxrwx     - jacob jacob 10 Jun 17:59   -I libspdlog.so.1.11 -&gt; libspdlog.so.1.11.0\n.rwxr-xr-x  748k jacob jacob 10 Jun 17:44   -I libspdlog.so.1.11.0\nlrwxrwxrwx     - jacob jacob 12 Jun 16:41   -I libtensorwrapper.so.0 -&gt; ./tensorwrapper/libtensorwrapper.so.0\nlrwxrwxrwx     - jacob jacob 12 Jun 16:41   -I libutilities.so.0 -&gt; ./utilities/libutilities.so.0\nlrwxrwxrwx     - jacob jacob 10 Jun 17:59   -I libxc.so -&gt; libxc.so.12\n.rwxr-xr-x   13M jacob jacob 10 Jun 17:45   -I libxc.so.12\n</code></pre> <p>Now that we have the libraries more easily accessible, we can then set the <code>LD_LIBRARY_PATH</code> variable to this directory so that our Python modules can use them.</p> <p>Once all these steps are done, and the <code>PYTHONPATH</code> and <code>LD_LIBRARY_PATH</code> variables are set, this should result in a functioning Python interface.</p> <p>If you plan on using C++ to interact with NWChemEx, here is an example of how I make sure that my <code>main.cpp</code> file is compiled correctly:</p> <pre><code>CXXFLAGS = -std=c++23 -Wall -Wextra -g \\\n-I/home/jacob/Applications/NWChemEx/install/include \\\n-I/home/jacob/Applications/NWChemEx/build/_deps/scf-src/include \\\n-I/home/jacob/Applications/NWChemEx/build/_deps/nux-src/include \\\n-I/home/jacob/Environments/base/include \\\n# -I/nix/store/a8r6jizjba7g1n99fla78aban17qcf9h-openmpi-5.0.6-dev/include \\\n# -I/nix/store/ammv4hfx001g454rn0dlgibj1imn9rkw-boost-1.87.0-dev/include\n\nLDFLAGS = \\\n-L/home/jacob/Applications/NWChemEx/build/_deps/scf-build/ -lscf \\\n-L/home/jacob/Applications/NWChemEx/build/_deps/nux-build/ -lnux \\\n-L/home/jacob/Applications/NWChemEx/install/lib/utilities/ -lutilities \\\n-L/home/jacob/Applications/NWChemEx/install/lib/chemist/ -lchemist \\\n-L/home/jacob/Applications/NWChemEx/install/lib/pluginplay/ -lpluginplay \\\n-L/home/jacob/Applications/NWChemEx/install/lib/chemcache/ -lchemcache \\\n-L/home/jacob/Applications/NWChemEx/install/lib/tensorwrapper/ -ltensorwrapper \\\n-L/home/jacob/Applications/NWChemEx/install/lib/integrals/ -lintegrals\n\nscf: scf_double.cpp\n    @g++ $(CXXFLAGS) main.cpp -o executable_binary $(LDFLAGS)\n</code></pre>"},{"location":"spack_nwchem_install_guide/","title":"Spack NWChem Install Guide","text":"<p>Here is a short list of commands to get NWChem (hopefully) up and running on the HiPerGator cluster. The gist is that we are going to be using a package manager called Spack, which we will clone from GitHub to be able to use.</p> <p>Spack essentially is a \"cookbook\" of different ways to build different packages. It builds everything from source, except for the compilers, which you need to supply locally. We'll add the NWChem package to a Spack-managed environment, and afterwards all that is needed is to source the Spack environment setup script and activate the Spack environment. Then we can build the NWChem package.</p> <p>This will most likely take a while (around 2 hours), here are the instructions:</p> <pre><code># While on HiPerGator\n\n# Clones Spack repo\ngit clone --depth=2 https://github.com/spack/spack.git\n\n# Checks out the correct branch of Spack\ncd spack\ngit checkout v0.23.1\n\n# Sets environment variables for Spack (Similar to a python environment, but not exactly)\n# assuming you're in the spack/ folder\nsource ./share/spack/setup-env.sh\n\n# Create the Spack environment (can be named anything, I chose nwchem)\nspack env create nwchem\nspack env activate nwchem \n\n# Load compiler modules\nmodule load gcc\n\n# Help Spack find compilers\nspack compiler find\n\n# Install NWChem\nspack install -j 8 --add nwchem armci=mpi-pr +openmp %gcc ^amdblis threads=openmp ^amdlibflame ^amdscalapack ^openmpi fabrics=cma,ucx\n</code></pre> <p>The flags for the spack install command are the following:</p> <ul> <li><code>-j 8</code> : Specifies only using 8 cores (since this is on a login node)</li> <li><code>--add</code> : adds the package to the activated spack environment</li> <li><code>nwchem</code>  : specifies nwchem is to be installed</li> <li><code>armci=mpi-pr</code> : armci version to use for nwchem</li> <li><code>+openmp</code> : include the optional openmp setting for nwchem</li> <li><code>%gcc</code> : use the gcc compiler, aocc is not on HiPerGator</li> <li><code>^amdblis threads=openmp</code> : overrides the original nwchem specs with amdblis as a dependency with the threads option set to openmp</li> <li><code>^amdlibflame</code> : overrides the original nwchem specs with amdlibflame as a dependency</li> <li><code>^amdscalapack</code> : overrrides the original nwchem specs with amdscalapack as a dependency</li> <li><code>^openmpi fabrics=cma,ucx</code> : overrides the original nwchem specs with openmpi as a dependency, with fabrics settings set to cma, ucx.</li> </ul> <p>Info obtained mostly from the AMD website regarding installation options for Spack.</p>"}]}